{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df7450c-53f3-44c8-9745-a58234ba312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faa8e7-c42c-45ab-9a4d-fafb9fa7259d",
   "metadata": {},
   "source": [
    "# 自 Yahoo Finance 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2bbc84-416d-4b89-9814-291087cd848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yahoo finance有六個欄位，Open High Low Close Adjusted-Close Volume\n",
    "def get_historical_Data(ticker, start, end):\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "    data = pd.DataFrame()\n",
    "    for j in range(6):\n",
    "        data = pd.concat([data, pd.DataFrame(yf.download(ticker, start=start_date, \n",
    "                                                         end=end_date).iloc[:, j])], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e046580b-f524-4b1b-8d0d-0835568b351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_pct_change</th>\n",
       "      <th>High_pct_change</th>\n",
       "      <th>Low_pct_change</th>\n",
       "      <th>Close_pct_change</th>\n",
       "      <th>Volume_pct_change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-02-01</th>\n",
       "      <td>0.750186</td>\n",
       "      <td>0.761381</td>\n",
       "      <td>0.750186</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>-0.521033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-02</th>\n",
       "      <td>0.747705</td>\n",
       "      <td>0.753881</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>-0.581061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-03</th>\n",
       "      <td>0.751406</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.750173</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>1.629906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-04</th>\n",
       "      <td>0.755039</td>\n",
       "      <td>0.759918</td>\n",
       "      <td>0.735525</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.003967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-05</th>\n",
       "      <td>0.747727</td>\n",
       "      <td>0.751370</td>\n",
       "      <td>0.738010</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>-0.074130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-08</th>\n",
       "      <td>0.748941</td>\n",
       "      <td>0.755018</td>\n",
       "      <td>0.746511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-09</th>\n",
       "      <td>0.742865</td>\n",
       "      <td>0.742865</td>\n",
       "      <td>0.733141</td>\n",
       "      <td>-0.006949</td>\n",
       "      <td>-0.795169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-10</th>\n",
       "      <td>0.748941</td>\n",
       "      <td>0.752613</td>\n",
       "      <td>0.744046</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>2.108927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-11</th>\n",
       "      <td>0.751386</td>\n",
       "      <td>0.764830</td>\n",
       "      <td>0.751386</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>-0.948630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-02-12</th>\n",
       "      <td>0.746508</td>\n",
       "      <td>0.746508</td>\n",
       "      <td>0.735562</td>\n",
       "      <td>-0.007649</td>\n",
       "      <td>1.179487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open_pct_change  High_pct_change  Low_pct_change  \\\n",
       "Date                                                           \n",
       "1993-02-01         0.750186         0.761381        0.750186   \n",
       "1993-02-02         0.747705         0.753881        0.744000   \n",
       "1993-02-03         0.751406         0.768661        0.750173   \n",
       "1993-02-04         0.755039         0.759918        0.735525   \n",
       "1993-02-05         0.747727         0.751370        0.738010   \n",
       "1993-02-08         0.748941         0.755018        0.746511   \n",
       "1993-02-09         0.742865         0.742865        0.733141   \n",
       "1993-02-10         0.748941         0.752613        0.744046   \n",
       "1993-02-11         0.751386         0.764830        0.751386   \n",
       "1993-02-12         0.746508         0.746508        0.735562   \n",
       "\n",
       "            Close_pct_change  Volume_pct_change  \n",
       "Date                                             \n",
       "1993-02-01          0.007113          -0.521033  \n",
       "1993-02-02          0.002118          -0.581061  \n",
       "1993-02-03          0.010571           1.629906  \n",
       "1993-02-04          0.004184           0.003967  \n",
       "1993-02-05         -0.000695          -0.074130  \n",
       "1993-02-08          0.000000           0.211339  \n",
       "1993-02-09         -0.006949          -0.795169  \n",
       "1993-02-10          0.001399           2.108927  \n",
       "1993-02-11          0.004892          -0.948630  \n",
       "1993-02-12         -0.007649           1.179487  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_historical_Data('SPY', '1993-01-01', '2022-12-31')\n",
    "\n",
    "data['Open_pct_change'] = (data['Open'] - data['Adj Close'].shift())/data['Adj Close'].shift().fillna(0)\n",
    "data['High_pct_change'] = (data['High'] - data['Adj Close'].shift())/data['Adj Close'].shift().fillna(0)\n",
    "data['Low_pct_change'] = (data['Low'] - data['Adj Close'].shift())/data['Adj Close'].shift().fillna(0)\n",
    "data['Close_pct_change'] = data['Adj Close'].pct_change().fillna(0) #use adjusted close price\n",
    "data['Volume_pct_change'] = data['Volume'].pct_change().fillna(0)\n",
    "\n",
    "data = data[1:] #drop 1st row\n",
    "data = data[['Open_pct_change', 'High_pct_change', 'Low_pct_change', 'Close_pct_change','Volume_pct_change']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929e567-330f-4a8f-acd0-beccc2878939",
   "metadata": {},
   "source": [
    "# DataFrame to Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767827b3-c1be-4493-a11d-932c2d2f00b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c13637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features: torch.Size([7528, 35]), Shape of the Labels: torch.Size([7528])\n"
     ]
    }
   ],
   "source": [
    "Features = torch.tensor(data.values)\n",
    "Lable = torch.tensor(data['Open_pct_change'].values)\n",
    "\n",
    "#Build rolling window of the features using unfold+flatten\n",
    "X = Features.unfold(0, 7, 1).flatten(start_dim=1)[:-1].to(torch.float32).to(device)# [:-1] is to drop the last row\n",
    "y = Lable[7:].to(torch.float32).to(device)\n",
    "\n",
    "print(f'Shape of the Features: {X.shape}, Shape of the Labels: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354b3687-6c6e-41d7-9b11-114ed3e2c87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.5019e-01,  7.4771e-01,  7.5141e-01,  7.5504e-01,  7.4773e-01,\n",
       "         7.4894e-01,  7.4286e-01,  7.6138e-01,  7.5388e-01,  7.6866e-01,\n",
       "         7.5992e-01,  7.5137e-01,  7.5502e-01,  7.4286e-01,  7.5019e-01,\n",
       "         7.4400e-01,  7.5017e-01,  7.3553e-01,  7.3801e-01,  7.4651e-01,\n",
       "         7.3314e-01,  7.1132e-03,  2.1183e-03,  1.0571e-02,  4.1840e-03,\n",
       "        -6.9467e-04,  0.0000e+00, -6.9491e-03, -5.2103e-01, -5.8106e-01,\n",
       "         1.6299e+00,  3.9668e-03, -7.4130e-02,  2.1134e-01, -7.9517e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4a844c-4cae-4ba3-b19f-dbb033460c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7489)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5500e38-5c66-436f-883c-f31d8d3d4bd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bbaca8-e69b-46d7-8843-37e1fbf25b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7528, 35]), torch.Size([7528]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cbf424d-b4c5-465b-981d-a50f461b5dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5269, 1129, 1130, 5269, 1129, 1130)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, \n",
    "                                                  test_size=0.3,\n",
    "                                                  random_state=42) # make the random split reproducible\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, \n",
    "                                                  test_size=0.5,\n",
    "                                                  random_state=42)\n",
    "#70% Train, 15% Valid, 15% Test\n",
    "\n",
    "len(X_train), len(X_valid),len(X_test), len(y_train), len(y_valid), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1dc07-2205-49be-b390-0ae0e8a27858",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b0ae07-211a-4da7-92a2-6c840d75989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Put Features and Labels Together\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "valid_data = TensorDataset(X_valid, y_valid)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 64\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(valid_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbf0449-1f93-40ea-9d97-71af4dfa0fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0785,  0.0973,  0.0756,  ..., -0.2114, -0.0595, -0.1977],\n",
       "         [ 0.4886,  0.4815,  0.4772,  ...,  0.1367, -0.1661, -0.1086],\n",
       "         [ 0.7332,  0.7343,  0.7238,  ..., -0.4624,  2.6998, -0.7123],\n",
       "         ...,\n",
       "         [ 0.4348,  0.4400,  0.4370,  ...,  0.1015,  0.1985, -0.1014],\n",
       "         [ 0.2232,  0.2416,  0.2446,  ...,  0.4201, -0.3294,  0.2584],\n",
       "         [ 0.6191,  0.6114,  0.6172,  ..., -0.4674, -0.2044,  0.3582]]),\n",
       " torch.Size([64, 35]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first batch in our training data\n",
    "torch.manual_seed(42)\n",
    "first_batch = next(iter(train_dataloader))[0]\n",
    "\n",
    "first_batch, first_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3c7c2-e2b7-4953-8294-6cf4c2141528",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1967c52-08e8-4e1d-856b-e50cd7d3a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GetLSTMOutput(nn.Module): #To Ignore the second output of LSTM Layer\n",
    "    def forward(self, x):\n",
    "        out, _ = x\n",
    "        return out\n",
    "\n",
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features = 1),\n",
    "            ##CNN Layers##\n",
    "            #input shape (64, 35), 64:batch_size, 35: 5 features * 7 days\n",
    "            \n",
    "            nn.Conv1d(in_channels = 1, out_channels = 64, kernel_size = 7, stride = 7),\n",
    "            #(64, 35) -> (64, 64, 5)\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2, stride = 1),\n",
    "            #(64, 64, 5) -> (64, 64, 4)\n",
    "            \n",
    "            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            #(64, 64, 4) -> (64, 32, 4)\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            #(64, 32, 4) -> (64, 32, 2)\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            #(64, 32, 2) -> (64, 64) where the 64 on dim 0 is the batch_size\n",
    "            \n",
    "            ##BiLSTM Layers##\n",
    "            \n",
    "            nn.LSTM(input_size=64, hidden_size=64, bidirectional=True, batch_first=True),\n",
    "            GetLSTMOutput(),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(input_size=128, hidden_size=32, bidirectional=True, batch_first=True),\n",
    "            GetLSTMOutput(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) #fit the shape\n",
    "        x = self.layers(x) #pass through CNN Layers\n",
    "        return x.squeeze()\n",
    "    \n",
    "model_0 = CNNBiLSTM() #create an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755c44ac-8160-4f44-9e19-8f6ce1631bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNBiLSTM                                [64]                      --\n",
       "├─Sequential: 1-1                        [64, 1]                   --\n",
       "│    └─BatchNorm1d: 2-1                  [64, 1, 35]               2\n",
       "│    └─Conv1d: 2-2                       [64, 64, 5]               512\n",
       "│    └─ReLU: 2-3                         [64, 64, 5]               --\n",
       "│    └─MaxPool1d: 2-4                    [64, 64, 4]               --\n",
       "│    └─Conv1d: 2-5                       [64, 32, 4]               6,176\n",
       "│    └─ReLU: 2-6                         [64, 32, 4]               --\n",
       "│    └─MaxPool1d: 2-7                    [64, 32, 2]               --\n",
       "│    └─Flatten: 2-8                      [64, 64]                  --\n",
       "│    └─LSTM: 2-9                         [64, 128]                 66,560\n",
       "│    └─GetLSTMOutput: 2-10               [64, 128]                 --\n",
       "│    └─ReLU: 2-11                        [64, 128]                 --\n",
       "│    └─LSTM: 2-12                        [64, 64]                  41,472\n",
       "│    └─GetLSTMOutput: 2-13               [64, 64]                  --\n",
       "│    └─ReLU: 2-14                        [64, 64]                  --\n",
       "│    └─Linear: 2-15                      [64, 1]                   65\n",
       "==========================================================================================\n",
       "Total params: 114,787\n",
       "Trainable params: 114,787\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 716.88\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.35\n",
       "Params size (MB): 0.46\n",
       "Estimated Total Size (MB): 0.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[64, 35]) # do a test pass through of an example input size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ca71-c398-4e49-8de0-d65533cd8339",
   "metadata": {},
   "source": [
    "# Training Loop, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "216169c8-ed14-4888-8a81-86ab464c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ed6f49-bb2f-4b05-9efa-aa2627ef26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNBiLSTM().to(device)\n",
    "loss_fn = nn.HuberLoss(reduction = 'mean', delta = 1.0) #The Loss Function to train model\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.001 , amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1aef0b6-05b2-4a06-9578-50289ba3f20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0322bed05c0048cf817bfc8c0e2392b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.121...\n",
      "\n",
      "Train loss: 0.11795 | Train mse: 0.23590 | Valid loss: 0.12112 | Valid mse: 0.24225\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.121...\n",
      "\n",
      "Train loss: 0.11803 | Train mse: 0.23606 | Valid loss: 0.12076 | Valid mse: 0.24152\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11815 | Train mse: 0.23630 | Valid loss: 0.12049 | Valid mse: 0.24097\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11786 | Train mse: 0.23572 | Valid loss: 0.12015 | Valid mse: 0.24031\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11819 | Train mse: 0.23637 | Valid loss: 0.12102 | Valid mse: 0.24204\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11768 | Train mse: 0.23535 | Valid loss: 0.12084 | Valid mse: 0.24169\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11765 | Train mse: 0.23531 | Valid loss: 0.12117 | Valid mse: 0.24234\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11780 | Train mse: 0.23561 | Valid loss: 0.12035 | Valid mse: 0.24069\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11781 | Train mse: 0.23561 | Valid loss: 0.12046 | Valid mse: 0.24092\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11768 | Train mse: 0.23535 | Valid loss: 0.12093 | Valid mse: 0.24186\n",
      "Epoch: 10\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11778 | Train mse: 0.23555 | Valid loss: 0.12068 | Valid mse: 0.24136\n",
      "Epoch: 11\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11805 | Train mse: 0.23610 | Valid loss: 0.12075 | Valid mse: 0.24150\n",
      "Epoch: 12\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11798 | Train mse: 0.23596 | Valid loss: 0.12118 | Valid mse: 0.24237\n",
      "Epoch: 13\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11776 | Train mse: 0.23553 | Valid loss: 0.12011 | Valid mse: 0.24022\n",
      "Epoch: 14\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11798 | Train mse: 0.23597 | Valid loss: 0.12071 | Valid mse: 0.24143\n",
      "Epoch: 15\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11769 | Train mse: 0.23538 | Valid loss: 0.12046 | Valid mse: 0.24091\n",
      "Epoch: 16\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11792 | Train mse: 0.23584 | Valid loss: 0.12063 | Valid mse: 0.24125\n",
      "Epoch: 17\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11808 | Train mse: 0.23617 | Valid loss: 0.12018 | Valid mse: 0.24035\n",
      "Epoch: 18\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11789 | Train mse: 0.23578 | Valid loss: 0.12138 | Valid mse: 0.24276\n",
      "Epoch: 19\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11800 | Train mse: 0.23601 | Valid loss: 0.12053 | Valid mse: 0.24105\n",
      "Epoch: 20\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11802 | Train mse: 0.23604 | Valid loss: 0.12087 | Valid mse: 0.24174\n",
      "Epoch: 21\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11788 | Train mse: 0.23577 | Valid loss: 0.12094 | Valid mse: 0.24187\n",
      "Epoch: 22\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11797 | Train mse: 0.23594 | Valid loss: 0.12072 | Valid mse: 0.24145\n",
      "Epoch: 23\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11757 | Train mse: 0.23514 | Valid loss: 0.12098 | Valid mse: 0.24196\n",
      "Epoch: 24\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11777 | Train mse: 0.23553 | Valid loss: 0.12122 | Valid mse: 0.24244\n",
      "Epoch: 25\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11795 | Train mse: 0.23589 | Valid loss: 0.12070 | Valid mse: 0.24140\n",
      "Epoch: 26\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11804 | Train mse: 0.23607 | Valid loss: 0.12130 | Valid mse: 0.24260\n",
      "Epoch: 27\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11783 | Train mse: 0.23566 | Valid loss: 0.12064 | Valid mse: 0.24127\n",
      "Epoch: 28\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11822 | Train mse: 0.23644 | Valid loss: 0.12099 | Valid mse: 0.24198\n",
      "Epoch: 29\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11752 | Train mse: 0.23503 | Valid loss: 0.12106 | Valid mse: 0.24213\n",
      "Epoch: 30\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11787 | Train mse: 0.23574 | Valid loss: 0.12064 | Valid mse: 0.24127\n",
      "Epoch: 31\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11812 | Train mse: 0.23624 | Valid loss: 0.12082 | Valid mse: 0.24164\n",
      "Epoch: 32\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11778 | Train mse: 0.23557 | Valid loss: 0.12116 | Valid mse: 0.24233\n",
      "Epoch: 33\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11791 | Train mse: 0.23582 | Valid loss: 0.12066 | Valid mse: 0.24133\n",
      "Epoch: 34\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11778 | Train mse: 0.23557 | Valid loss: 0.12060 | Valid mse: 0.24120\n",
      "Epoch: 35\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11787 | Train mse: 0.23574 | Valid loss: 0.12126 | Valid mse: 0.24253\n",
      "Epoch: 36\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11786 | Train mse: 0.23572 | Valid loss: 0.12049 | Valid mse: 0.24098\n",
      "Epoch: 37\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11772 | Train mse: 0.23544 | Valid loss: 0.12087 | Valid mse: 0.24175\n",
      "Epoch: 38\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11770 | Train mse: 0.23541 | Valid loss: 0.12077 | Valid mse: 0.24154\n",
      "Epoch: 39\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11777 | Train mse: 0.23554 | Valid loss: 0.12071 | Valid mse: 0.24142\n",
      "Epoch: 40\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11769 | Train mse: 0.23537 | Valid loss: 0.12081 | Valid mse: 0.24163\n",
      "Epoch: 41\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11801 | Train mse: 0.23602 | Valid loss: 0.12029 | Valid mse: 0.24058\n",
      "Epoch: 42\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11809 | Train mse: 0.23619 | Valid loss: 0.12082 | Valid mse: 0.24164\n",
      "Epoch: 43\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11784 | Train mse: 0.23568 | Valid loss: 0.12004 | Valid mse: 0.24009\n",
      "Epoch: 44\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11815 | Train mse: 0.23630 | Valid loss: 0.12074 | Valid mse: 0.24148\n",
      "Epoch: 45\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11826 | Train mse: 0.23652 | Valid loss: 0.12103 | Valid mse: 0.24206\n",
      "Epoch: 46\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11836 | Train mse: 0.23673 | Valid loss: 0.12059 | Valid mse: 0.24118\n",
      "Epoch: 47\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11752 | Train mse: 0.23504 | Valid loss: 0.12067 | Valid mse: 0.24134\n",
      "Epoch: 48\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11793 | Train mse: 0.23587 | Valid loss: 0.12132 | Valid mse: 0.24264\n",
      "Epoch: 49\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11806 | Train mse: 0.23612 | Valid loss: 0.12077 | Valid mse: 0.24154\n",
      "Epoch: 50\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11779 | Train mse: 0.23558 | Valid loss: 0.12128 | Valid mse: 0.24257\n",
      "Epoch: 51\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11815 | Train mse: 0.23631 | Valid loss: 0.12126 | Valid mse: 0.24253\n",
      "Epoch: 52\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11774 | Train mse: 0.23549 | Valid loss: 0.12091 | Valid mse: 0.24181\n",
      "Epoch: 53\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11757 | Train mse: 0.23514 | Valid loss: 0.12110 | Valid mse: 0.24219\n",
      "Epoch: 54\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11805 | Train mse: 0.23610 | Valid loss: 0.12109 | Valid mse: 0.24219\n",
      "Epoch: 55\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11793 | Train mse: 0.23586 | Valid loss: 0.12048 | Valid mse: 0.24096\n",
      "Epoch: 56\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11786 | Train mse: 0.23573 | Valid loss: 0.12071 | Valid mse: 0.24142\n",
      "Epoch: 57\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11775 | Train mse: 0.23549 | Valid loss: 0.12087 | Valid mse: 0.24174\n",
      "Epoch: 58\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11792 | Train mse: 0.23583 | Valid loss: 0.12021 | Valid mse: 0.24042\n",
      "Epoch: 59\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11791 | Train mse: 0.23582 | Valid loss: 0.12118 | Valid mse: 0.24236\n",
      "Epoch: 60\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11781 | Train mse: 0.23561 | Valid loss: 0.11962 | Valid mse: 0.23923\n",
      "Epoch: 61\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11790 | Train mse: 0.23580 | Valid loss: 0.12084 | Valid mse: 0.24169\n",
      "Epoch: 62\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11783 | Train mse: 0.23567 | Valid loss: 0.12117 | Valid mse: 0.24235\n",
      "Epoch: 63\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11779 | Train mse: 0.23559 | Valid loss: 0.12051 | Valid mse: 0.24103\n",
      "Epoch: 64\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11755 | Train mse: 0.23510 | Valid loss: 0.12070 | Valid mse: 0.24140\n",
      "Epoch: 65\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11810 | Train mse: 0.23621 | Valid loss: 0.12065 | Valid mse: 0.24130\n",
      "Epoch: 66\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11787 | Train mse: 0.23574 | Valid loss: 0.12084 | Valid mse: 0.24168\n",
      "Epoch: 67\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11767 | Train mse: 0.23534 | Valid loss: 0.12099 | Valid mse: 0.24198\n",
      "Epoch: 68\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11786 | Train mse: 0.23572 | Valid loss: 0.12061 | Valid mse: 0.24123\n",
      "Epoch: 69\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11787 | Train mse: 0.23574 | Valid loss: 0.12015 | Valid mse: 0.24029\n",
      "Epoch: 70\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11776 | Train mse: 0.23552 | Valid loss: 0.12086 | Valid mse: 0.24172\n",
      "Epoch: 71\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11770 | Train mse: 0.23540 | Valid loss: 0.12050 | Valid mse: 0.24100\n",
      "Epoch: 72\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11797 | Train mse: 0.23593 | Valid loss: 0.12041 | Valid mse: 0.24082\n",
      "Epoch: 73\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11786 | Train mse: 0.23573 | Valid loss: 0.12101 | Valid mse: 0.24201\n",
      "Epoch: 74\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11777 | Train mse: 0.23555 | Valid loss: 0.12122 | Valid mse: 0.24245\n",
      "Epoch: 75\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11769 | Train mse: 0.23537 | Valid loss: 0.12067 | Valid mse: 0.24134\n",
      "Epoch: 76\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11787 | Train mse: 0.23573 | Valid loss: 0.12057 | Valid mse: 0.24113\n",
      "Epoch: 77\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11802 | Train mse: 0.23603 | Valid loss: 0.12146 | Valid mse: 0.24292\n",
      "Epoch: 78\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11780 | Train mse: 0.23559 | Valid loss: 0.12093 | Valid mse: 0.24186\n",
      "Epoch: 79\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11792 | Train mse: 0.23584 | Valid loss: 0.12105 | Valid mse: 0.24210\n",
      "Epoch: 80\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11768 | Train mse: 0.23536 | Valid loss: 0.12097 | Valid mse: 0.24193\n",
      "Epoch: 81\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "Saving model with loss 0.120...\n",
      "\n",
      "Train loss: 0.11778 | Train mse: 0.23556 | Valid loss: 0.11957 | Valid mse: 0.23914\n",
      "Epoch: 82\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11767 | Train mse: 0.23534 | Valid loss: 0.12046 | Valid mse: 0.24093\n",
      "Epoch: 83\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11775 | Train mse: 0.23550 | Valid loss: 0.12091 | Valid mse: 0.24181\n",
      "Epoch: 84\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11809 | Train mse: 0.23619 | Valid loss: 0.12033 | Valid mse: 0.24066\n",
      "Epoch: 85\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11771 | Train mse: 0.23542 | Valid loss: 0.12067 | Valid mse: 0.24134\n",
      "Epoch: 86\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11767 | Train mse: 0.23534 | Valid loss: 0.12122 | Valid mse: 0.24244\n",
      "Epoch: 87\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11807 | Train mse: 0.23614 | Valid loss: 0.12034 | Valid mse: 0.24068\n",
      "Epoch: 88\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11804 | Train mse: 0.23609 | Valid loss: 0.12063 | Valid mse: 0.24126\n",
      "Epoch: 89\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11780 | Train mse: 0.23560 | Valid loss: 0.12150 | Valid mse: 0.24300\n",
      "Epoch: 90\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11766 | Train mse: 0.23532 | Valid loss: 0.12092 | Valid mse: 0.24183\n",
      "Epoch: 91\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11784 | Train mse: 0.23568 | Valid loss: 0.12097 | Valid mse: 0.24194\n",
      "Epoch: 92\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11790 | Train mse: 0.23580 | Valid loss: 0.12035 | Valid mse: 0.24071\n",
      "Epoch: 93\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11783 | Train mse: 0.23567 | Valid loss: 0.12035 | Valid mse: 0.24070\n",
      "Epoch: 94\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11781 | Train mse: 0.23563 | Valid loss: 0.12103 | Valid mse: 0.24206\n",
      "Epoch: 95\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11827 | Train mse: 0.23655 | Valid loss: 0.12033 | Valid mse: 0.24067\n",
      "Epoch: 96\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11799 | Train mse: 0.23599 | Valid loss: 0.12076 | Valid mse: 0.24152\n",
      "Epoch: 97\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11779 | Train mse: 0.23559 | Valid loss: 0.12063 | Valid mse: 0.24125\n",
      "Epoch: 98\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11762 | Train mse: 0.23524 | Valid loss: 0.12103 | Valid mse: 0.24207\n",
      "Epoch: 99\n",
      "-------\n",
      "Looked at 0/5269 samples\n",
      "Looked at 2560/5269 samples\n",
      "Looked at 5120/5269 samples\n",
      "\n",
      "Train loss: 0.11764 | Train mse: 0.23528 | Valid loss: 0.12110 | Valid mse: 0.24220\n",
      "Train time on cpu: 54.636 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 100\n",
    "model = CNNBiLSTM().to(device)\n",
    "\n",
    "best_loss = math.inf\n",
    "\n",
    "if not os.path.isdir('./models'):\n",
    "    os.mkdir('./models') # Create directory of saving models.\n",
    "    \n",
    "model_save_path = './models/model.ckpt'\n",
    "\n",
    "# Create training and validating loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    train_mse = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch\n",
    "        train_mse += mse_loss(y_pred, y)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 40 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_mse /= len(train_dataloader)\n",
    "    \n",
    "    ### Validating\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    valid_loss = 0 \n",
    "    valid_mse = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in valid_dataloader:\n",
    "            # 1. Forward pass\n",
    "            valid_pred = model(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            valid_loss += loss_fn(valid_pred, y) # accumulatively add up the loss per epoch\n",
    "            valid_mse += mse_loss(valid_pred, y)\n",
    "        \n",
    "        # Calculations on valid metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total valid loss by length of valid dataloader (per batch)\n",
    "        valid_loss /= len(valid_dataloader)\n",
    "        valid_mse /= len(valid_dataloader)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_save_path) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Train mse: {train_mse:.5f} | Valid loss: {valid_loss:.5f} | Valid mse: {valid_mse:.5f}\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfb3d0-5d31-44ee-af7a-4e7bceedc2f0",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbcea8d2-6b11-4870-bb50-89733415df0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Trained Model\n",
    "model_test = CNNBiLSTM().to(device)\n",
    "model_test.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5dfda68-9106-4df4-909d-92e22afc94a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1130, 35]), torch.Size([1130]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the Test Set\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543f89c4-c809-4987-83fd-9ef9fd67c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE Loss on Test Set is: 0.24560736119747162\n"
     ]
    }
   ],
   "source": [
    "model_test.eval()\n",
    "with torch.inference_mode():\n",
    "    test_pred = model_test(X_test)\n",
    "    print(f'The MSE Loss on Test Set is: {mse_loss(test_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849ea809-0e7d-4fb6-984e-64b24255594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098429</td>\n",
       "      <td>0.045009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.097440</td>\n",
       "      <td>0.062112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.096896</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.096304</td>\n",
       "      <td>0.407399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.096385</td>\n",
       "      <td>0.116513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>-0.098368</td>\n",
       "      <td>0.004340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-0.097561</td>\n",
       "      <td>0.367939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>-0.094988</td>\n",
       "      <td>0.360005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>-0.091011</td>\n",
       "      <td>0.660597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>-0.089910</td>\n",
       "      <td>0.160156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pred      real\n",
       "0    -0.098429  0.045009\n",
       "1    -0.097440  0.062112\n",
       "2    -0.096896  0.553900\n",
       "3    -0.096304  0.407399\n",
       "4    -0.096385  0.116513\n",
       "...        ...       ...\n",
       "1125 -0.098368  0.004340\n",
       "1126 -0.097561  0.367939\n",
       "1127 -0.094988  0.360005\n",
       "1128 -0.091011  0.660597\n",
       "1129 -0.089910  0.160156\n",
       "\n",
       "[1130 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_pred).rename(columns = {0 : 'pred'})\n",
    "pd.DataFrame(y_test).rename(columns = {0 : 'real'})\n",
    "df = pd.concat([pd.DataFrame(test_pred).rename(columns = {0 : 'pred'}),\n",
    "                pd.DataFrame(y_test).rename(columns = {0 : 'real'})],\n",
    "                axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b61b75-1b0d-4e81-8448-f33ddad869a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pred = df['pred'].values\n",
    "real = df['real'].values\n",
    "plt.scatter(pred, real)\n",
    "\n",
    "# Add 45 degree line\n",
    "x = np.linspace(min(pred.min(), real.min()), max(pred.max(), real.max()), 100)\n",
    "plt.plot(x, x, color='red', linestyle='--')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Real Values')\n",
    "plt.title('Scatter Plot with 45 Degree Line')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a67c6e9-3d0e-4ecf-96b4-52b81992932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.09503292, 0.0026163564, 0.3509364, 0.21658629)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean(), pred.std(), real.mean(), real.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad2168-c565-48b6-b372-73cef8396351",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 結論\n",
    "\n",
    "使用過往七日OHLC變化率加上成交量變化率作為輸入，透過CNN-BiLSTM模型預測隔日的開盤價變動率，在Test Set上的MSE Loss表現與Train Set, Valid Set差不多。\n",
    "\n",
    "但繪圖之後我們能發現，該模型的預測能力很差，判斷為Underfit，故之後會加入多個資產的資料進行訓練。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
